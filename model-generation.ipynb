{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c412925-a69b-44ef-955b-9c805fa466e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from cleaning import clean_lyrics_df\n",
    "\n",
    "# Purpose: AI Lyricist Project\n",
    "# This project aims to generate music lyrics using machine learning models. The system is designed to train on a dataset of song lyrics, learn patterns in word sequences, and generate new, coherent lyrics based on a seed phrase provided by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfcd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1bb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a694fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "# Import tokenizers\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from cleaning import clean_lyrics_df\n",
    "\n",
    "# Import nltk and download  the sentence tokenizer.\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0080488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alejandro-\n",
      "[nltk_data]     germosen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alejandro-\n",
      "[nltk_data]     germosen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/alejandro-\n",
      "[nltk_data]     germosen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /Users/alejandro-\n",
      "[nltk_data]     germosen/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3c81337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(df):\n",
    "    s_stemmer = SnowballStemmer(language='english')\n",
    "    # Tokenize the words \n",
    "    print(\"snoopy\", df)\n",
    "    text = df.loc[0, \"lyrics\"]\n",
    "    words = word_tokenize(text)\n",
    "    # Get the stem of the words\n",
    "    stem = [s_stemmer.stem(word) for word in words]\n",
    "    # Get the stopwords\n",
    "    sw = set(stopwords.words('english'))\n",
    "    # Retrieve only the words that aren't in the stopwords.\n",
    "    output = [word.lower() for word in stem if word.lower() not in sw]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba7f63f2-2ed9-4d90-9c7d-ba32929891bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Preprocess Data\n",
    "\n",
    "def load_lyrics(file_path, chunksize=10000, limit=None):\n",
    "    \"\"\"\n",
    "    Load and process a large CSV file in chunks.\n",
    "\n",
    "    :param file_path: Path to the CSV file.\n",
    "    :param chunksize: Number of rows per chunk to process.\n",
    "    :return: Combined lyrics as a single text block.\n",
    "    \"\"\"\n",
    "    lyrics_list = []\n",
    "    rows_count = 0\n",
    "\n",
    "    # Read the CSV file in chunks\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Drop rows with missing lyrics in the current chunk\n",
    "        chunk = clean_lyrics_df(chunk)\n",
    "        chunk = chunk.dropna(subset=['lyrics'])\n",
    "        # Append the lyrics to the list\n",
    "        print(f\"Chunk shape: {chunk}\")\n",
    "        s = stop_words(chunk)\n",
    "        print(f\"Chunk stop: {s}\")\n",
    "        lyrics_list.extend(s)\n",
    "\n",
    "        if limit and rows_count >= limit:\n",
    "            break\n",
    "        else:\n",
    "            rows_count += chunk.shape[0]\n",
    "\n",
    "    print(f\"Total lyrics loaded: {len(lyrics_list)}\")\n",
    "    print(f\"Rows limit: {rows_count}\")\n",
    "    # Combine all lyrics into a single text block\n",
    "    lyrics = \"\\n\".join(lyrics_list)\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139e89ee-19fd-458e-8971-cda8855848b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Sequence Preparation\n",
    "def preprocess_text(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Create input sequences\n",
    "    input_sequences = []\n",
    "    for line in text.split('\\n'):\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # Pad sequences\n",
    "    max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # Split into predictors and label\n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "    label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "    return predictors, label, max_sequence_len, total_words, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93996470-a019-4a58-a3f7-1dd4ea6d783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the Model\n",
    "def build_model(total_words, max_sequence_len):\n",
    "    model = Sequential([\n",
    "        Embedding(total_words, 64, input_length=max_sequence_len-1),\n",
    "        LSTM(100, return_sequences=True),\n",
    "        LSTM(100),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(total_words, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89243f05-54cb-492d-bab1-6f72dff4cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train the Model\n",
    "def train_model(model, predictors, label, epochs=30):\n",
    "    model.fit(predictors, label, epochs=epochs, verbose=1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e0a08d4-6c9a-4c3e-8bd4-82b4aa1892c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Lyrics\n",
    "def generate_lyrics(seed_text, model, tokenizer, max_sequence_len, num_words=20):\n",
    "    for _ in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                seed_text += \" \" + word\n",
    "                break\n",
    "\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "468b0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(df):\n",
    "    s_stemmer = SnowballStemmer(language='english')\n",
    "    # Tokenize the words \n",
    "    text = df.loc[0, \"lyrics\"]\n",
    "    print(\"snoopy\", df)\n",
    "    words = word_tokenize(text)\n",
    "    # Get the stem of the words\n",
    "    stem = [s_stemmer.stem(word) for word in words]\n",
    "    # Get the stopwords\n",
    "    sw = set(stopwords.words('english'))\n",
    "    # Retrieve only the words that aren't in the stopwords.\n",
    "    output = [word.lower() for word in stem if word.lower() not in sw]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00f8cd23-fc97-42fd-bf99-c5ef7f16c964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape:    index             song  year           artist genre  \\\n",
      "0      0        ego-remix  2009  beyonce-knowles   Pop   \n",
      "1      1     then-tell-me  2009  beyonce-knowles   Pop   \n",
      "2      2          honesty  2009  beyonce-knowles   Pop   \n",
      "3      3  you-are-my-rock  2009  beyonce-knowles   Pop   \n",
      "4      4    black-culture  2009  beyonce-knowles   Pop   \n",
      "\n",
      "                                              lyrics  \n",
      "0  Oh baby, how you doing? You know I am gonna cu...  \n",
      "1  playin everything so easy, it is like you seem...  \n",
      "2  If you search For tenderness It is not hard to...  \n",
      "3  Oh oh oh I, oh oh oh I [Verse 1:] If I wrote a...  \n",
      "4  Party the people, the people the party it is p...  \n",
      "snoopy    index             song  year           artist genre  \\\n",
      "0      0        ego-remix  2009  beyonce-knowles   Pop   \n",
      "1      1     then-tell-me  2009  beyonce-knowles   Pop   \n",
      "2      2          honesty  2009  beyonce-knowles   Pop   \n",
      "3      3  you-are-my-rock  2009  beyonce-knowles   Pop   \n",
      "4      4    black-culture  2009  beyonce-knowles   Pop   \n",
      "\n",
      "                                              lyrics  \n",
      "0  Oh baby, how you doing? You know I am gonna cu...  \n",
      "1  playin everything so easy, it is like you seem...  \n",
      "2  If you search For tenderness It is not hard to...  \n",
      "3  Oh oh oh I, oh oh oh I [Verse 1:] If I wrote a...  \n",
      "4  Party the people, the people the party it is p...  \n",
      "Chunk stop: ['oh', 'babi', ',', '?', 'know', 'gon', 'na', 'cut', 'right', 'chase', 'women', 'made', ',', 'like', 'think', 'creat', 'special', 'purpos', 'know', ',', 'special', '?', 'feel', 'babi', ',', 'let', 'get', 'lost', 'need', 'call', 'work', 'caus', 'boss', 'real', ',', 'want', 'show', 'feel', 'consid', 'lucki', ',', 'big', 'deal', 'whi', '?', 'well', ',', 'got', 'key', 'heart', 'ai', 'gon', 'na', 'need', ',', 'id', 'rather', 'open', 'bodi', 'show', 'secret', ',', 'know', 'insid', 'need', 'lie', 'big', ',', 'wide', 'strong', ',', 'wo', 'fit', 'much', ',', 'tough', 'talk', 'like', 'caus', 'back', 'got', 'big', 'ego', ',', 'huge', 'ego', 'love', 'big', 'ego', ',', 'much', 'walk', 'like', 'caus', 'back', 'usual', 'humbl', ',', 'right', 'choos', 'leav', 'could', 'blue', 'call', 'arrog', ',', 'call', 'confid', 'decid', 'find', 'work', 'damn', 'know', 'kill', 'leg', 'better', 'yet', 'thigh', 'matter', 'fact', 'smile', 'mayb', 'eye', 'boy', 'site', 'see', ',', 'kind', 'someth', 'like', 'big', ',', 'wide', 'strong', ',', 'wo', 'fit', 'much', ',', 'tough', 'talk', 'like', 'caus', 'back', 'got', 'big', 'ego', ',', 'huge', 'ego', 'love', 'big', 'ego', ',', 'much', 'walk', 'like', 'caus', 'back', ',', 'walk', 'like', 'caus', 'back', ',', 'talk', 'like', 'caus', 'back', ',', 'back', ',', 'back', 'walk', 'like', 'caus', 'back', 'big', ',', 'wide', 'strong', ',', 'wo', 'fit', 'much', ',', 'tough', 'talk', 'like', 'caus', 'back', 'got', 'big', 'ego', ',', 'huge', 'ego', ',', 'huge', 'ego', 'love', 'big', 'ego', ',', 'much', 'walk', 'like', 'caus', 'back', 'ego', 'big', ',', 'must', 'admit', 'got', 'everi', 'reason', 'feel', 'like', 'bitch', 'ego', 'strong', ',', 'ai', 'know', 'need', 'beat', ',', 'sing', 'piano']\n",
      "Chunk shape:    index                    song  year           artist genre  \\\n",
      "5      5  all-i-could-do-was-cry  2009  beyonce-knowles   Pop   \n",
      "6      6      once-in-a-lifetime  2009  beyonce-knowles   Pop   \n",
      "7      7                 waiting  2009  beyonce-knowles   Pop   \n",
      "8      8               slow-love  2009  beyonce-knowles   Pop   \n",
      "9      9   why-don-t-you-love-me  2009  beyonce-knowles   Pop   \n",
      "\n",
      "                                              lyrics  \n",
      "5  I heard Church bells ringing I heard A choir s...  \n",
      "6  This is just another day that I would spend Wa...  \n",
      "7  Waiting, waiting, waiting, waiting Waiting, wa...  \n",
      "8  [Verse 1:] I read all of the magazines while w...  \n",
      "9  N-n-now, honey You better sit down and look ar...  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/indexes/range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Resources/lyrics_chunks/lyrics_1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_path):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     lyrics \u001b[38;5;241m=\u001b[39m \u001b[43mload_lyrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocess_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     predictors, label, max_sequence_len, total_words, tokenizer \u001b[38;5;241m=\u001b[39m preprocess_text(lyrics)\n",
      "Cell \u001b[0;32mIn[42], line 21\u001b[0m, in \u001b[0;36mload_lyrics\u001b[0;34m(file_path, chunksize, limit)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Append the lyrics to the list\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mstop_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk stop: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m lyrics_list\u001b[38;5;241m.\u001b[39mextend(s)\n",
      "Cell \u001b[0;32mIn[55], line 4\u001b[0m, in \u001b[0;36mstop_words\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m s_stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Tokenize the words \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlyrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnoopy\u001b[39m\u001b[38;5;124m\"\u001b[39m, df)\n\u001b[1;32m      6\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/indexing.py:1183\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/frame.py:4221\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4215\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[1;32m   4217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   4218\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   4219\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   4220\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[0;32m-> 4221\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[1;32m   4224\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[1;32m   4225\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/indexes/range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# # Main Workflow\n",
    "\n",
    "# File path to lyrics dataset (replace with actual file path)\n",
    "data_path = \"./Resources/lyrics_chunks/lyrics_1.csv\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    # Load and preprocess data\n",
    "    lyrics = load_lyrics(data_path, chunksize=5, limit=15)\n",
    "    print(\"preprocess_text\")\n",
    "    predictors, label, max_sequence_len, total_words, tokenizer = preprocess_text(lyrics)\n",
    "\n",
    "    # Build and train the model\n",
    "    print(\"build_model\")\n",
    "    model = build_model(total_words, max_sequence_len)\n",
    "    print(\"train_model\")\n",
    "    model = train_model(model, predictors, label, epochs=10)\n",
    "else:\n",
    "    print(f\"Dataset not found at {data_path}. Please provide a valid path. Try going through the cleaning.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4865cf20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 47), found shape=(None, 29)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# generate_lyrics(seed_text=\"roses are red, violets blue, I'll love it to wed...\", model=model, tokenizer=tokenizer, max_sequence_len=max_sequence_len, num_words=20)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_lyrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroses are red\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mgenerate_lyrics\u001b[0;34m(seed_text, model, tokenizer, max_sequence_len, num_words)\u001b[0m\n\u001b[1;32m      4\u001b[0m token_list \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([seed_text])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m token_list \u001b[38;5;241m=\u001b[39m pad_sequences([token_list], maxlen\u001b[38;5;241m=\u001b[39mmax_sequence_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m predicted \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_list\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, index \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mword_index\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m==\u001b[39m predicted:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/0g/m_btl88n5hs7w95j59p7v2180000gn/T/__autograph_generated_filevgdnm708.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 47), found shape=(None, 29)\n"
     ]
    }
   ],
   "source": [
    "# generate_lyrics(seed_text=\"roses are red, violets blue, I'll love it to wed...\", model=model, tokenizer=tokenizer, max_sequence_len=max_sequence_len, num_words=20)\n",
    "generate_lyrics(seed_text=\"roses are red\", model=model, tokenizer=tokenizer, max_sequence_len=30, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c432b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lyrics(input_text):\n",
    "    generated_lyrics = generate_lyrics(seed_text=input_text, model=model, tokenizer=tokenizer, max_sequence_len=30, num_words=20)\n",
    "    return generated_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eaf2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372aed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59bde6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lyrics_genie.keras')\n",
    "dump(tokenizer, open('lyrics_genie', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42381eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 20:12:59.983204: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-08 20:12:59.983955: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-08 20:12:59.984893: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-01-08 20:13:00.066285: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-01-08 20:13:00.067227: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-01-08 20:13:00.067660: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model = load_model('lyrics_genie.keras')\n",
    "tokenizer = load(open('lyrics_genie', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2be66e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.40.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/blocks.py\", line 1923, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/blocks.py\", line 1508, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/utils.py\", line 818, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/0g/m_btl88n5hs7w95j59p7v2180000gn/T/ipykernel_56724/549057648.py\", line 2, in predict_lyrics\n",
      "    generated_lyrics = generate_lyrics(seed_text=input_text, model=model, tokenizer=tokenizer, max_sequence_len=30, num_words=20)\n",
      "  File \"/var/folders/0g/m_btl88n5hs7w95j59p7v2180000gn/T/ipykernel_56724/3105462436.py\", line 6, in generate_lyrics\n",
      "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
      "  File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/var/folders/0g/m_btl88n5hs7w95j59p7v2180000gn/T/__autograph_generated_filevgdnm708.py\", line 15, in tf__predict_function\n",
      "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 47), found shape=(None, 29)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from gradio import gr\n",
    "import gradio as gr\n",
    "    # Launch Gradio Interface\n",
    "interface = gr.Interface(fn=predict_lyrics, \n",
    "                            inputs=\"text\", \n",
    "                            outputs=\"text\", \n",
    "                            title=\"AI Lyric Generator\",\n",
    "                            description=\"Enter a seed phrase to generate music lyrics.\")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totality: 266557 -> 0\n",
      "With Verse: 3915 -> 0\n"
     ]
    }
   ],
   "source": [
    "# show all lyrics that have the word 'verse'\n",
    "totality = len(lyrics)\n",
    "with_verse = len(lyrics[lyrics['lyrics'].str.contains('\\[verse', case=False)])\n",
    "replaceLyricsChars('\\[verse+\\]', '', regex=True)\n",
    "replaceLyricsChars('\\[verse*\\]', '', regex=True)\n",
    "\n",
    "delta_totality = totality - len(lyrics)\n",
    "delta_with_verse = with_verse - len(lyrics[lyrics['lyrics'].str.contains('\\[verse', case=False)])\n",
    "\n",
    "print(f\"Totality: {totality} -> {delta_totality}\")\n",
    "print(f\"With Verse: {with_verse} -> {delta_with_verse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd7a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totality: 266557 -> 0\n",
      "With intro: 735 -> 0\n"
     ]
    }
   ],
   "source": [
    "# show all lyrics that have the word 'intro'\n",
    "totality = len(lyrics)\n",
    "with_intro = len(lyrics[lyrics['lyrics'].str.contains('\\[intro', case=False)])\n",
    "replaceLyricsChars('\\[intro+\\]', '', regex=True)\n",
    "replaceLyricsChars('\\[intro*\\]', '', regex=True)\n",
    "\n",
    "delta_totality = totality - len(lyrics)\n",
    "delta_with_intro = with_intro - len(lyrics[lyrics['lyrics'].str.contains('\\[intro', case=False)])\n",
    "\n",
    "print(f\"Totality: {totality} -> {delta_totality}\")\n",
    "print(f\"With intro: {with_intro} -> {delta_with_intro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff85da8f-6e52-4e4b-b845-d08c59bdaad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0g/m_btl88n5hs7w95j59p7v2180000gn/T/ipykernel_38805/2075172157.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Load and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpredictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Build and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/0g/m_btl88n5hs7w95j59p7v2180000gn/T/ipykernel_38805/1940501176.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Create input sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    289\u001b[0m                         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    294\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                         \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predictors, label, max_sequence_len, total_words, tokenizer = preprocess_text(lyrics)\n",
    "\n",
    "# Build and train the model\n",
    "model = build_model(total_words, max_sequence_len)\n",
    "model = train_model(model, predictors, label, epochs=30)\n",
    "\n",
    "# Generate new lyrics\n",
    "seed_text = \"love is a beautiful\"\n",
    "generated_lyrics = generate_lyrics(seed_text, model, tokenizer, max_sequence_len, num_words=50)\n",
    "print(\"Generated Lyrics:\")\n",
    "print(generated_lyrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
